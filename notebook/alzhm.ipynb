{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5962731,"sourceType":"datasetVersion","datasetId":3419493}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport tensorflow as tf\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nimport cv2\nimport os\nfrom PIL import Image\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nprint(\"‚úÖ All libraries imported successfully!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:47:36.749147Z","iopub.execute_input":"2025-12-09T13:47:36.749469Z","iopub.status.idle":"2025-12-09T13:47:36.758043Z","shell.execute_reply.started":"2025-12-09T13:47:36.749445Z","shell.execute_reply":"2025-12-09T13:47:36.756935Z"}},"outputs":[{"name":"stdout","text":"‚úÖ All libraries imported successfully!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# SAFE CLEAR SESSION\nimport tensorflow as tf\ntf.keras.backend.clear_session()\nimport gc\ngc.collect()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:47:36.759936Z","iopub.execute_input":"2025-12-09T13:47:36.760221Z","iopub.status.idle":"2025-12-09T13:47:37.202817Z","shell.execute_reply.started":"2025-12-09T13:47:36.760200Z","shell.execute_reply":"2025-12-09T13:47:37.201805Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"\nclass Config:\n    IMG_SIZE = (224, 224)\n    BATCH_SIZE = 8\n    EPOCHS = 4\n    NUM_CLASSES = 4\n    LEARNING_RATE = 0.0001\n    DATA_PATH = '/kaggle/input/imagesoasis/Data' \n\nconfig = Config()\n\ndef check_dataset_structure():\n    if os.path.exists(config.DATA_PATH):\n        classes = os.listdir(config.DATA_PATH)\n        print(\"üìÅ Dataset structure:\")\n        for class_name in classes:\n            class_path = os.path.join(config.DATA_PATH, class_name)\n            if os.path.isdir(class_path):\n                num_images = len([f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))])\n                print(f\"   {class_name}: {num_images} images\")\n    else:\n        print(\"‚ùå Dataset path not found. Please check the path.\")\n\ncheck_dataset_structure()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:47:37.203827Z","iopub.execute_input":"2025-12-09T13:47:37.204150Z","iopub.status.idle":"2025-12-09T13:47:38.493315Z","shell.execute_reply.started":"2025-12-09T13:47:37.204124Z","shell.execute_reply":"2025-12-09T13:47:38.491862Z"}},"outputs":[{"name":"stdout","text":"üìÅ Dataset structure:\n   Non Demented: 67222 images\n   Very mild Dementia: 13725 images\n   Moderate Dementia: 488 images\n   Mild Dementia: 5002 images\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"\ndef create_data_generators():\n    train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=15,  \n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        horizontal_flip=True,\n        zoom_range=0.2,\n        brightness_range=[0.8, 1.2],\n        validation_split=0.2 \n    )\n    \n    test_datagen = ImageDataGenerator(rescale=1./255)\n    \n    train_generator = train_datagen.flow_from_directory(\n        config.DATA_PATH,\n        target_size=config.IMG_SIZE,\n        batch_size=config.BATCH_SIZE,\n        class_mode='categorical',\n        subset='training',\n        shuffle=True\n    )\n    \n    val_generator = train_datagen.flow_from_directory(\n        config.DATA_PATH,\n        target_size=config.IMG_SIZE,\n        batch_size=config.BATCH_SIZE,\n        class_mode='categorical',\n        subset='validation',\n        shuffle=False\n    )\n    \n    return train_generator, val_generator\n\nprint(\"üîÑ Creating data generators...\")\ntrain_generator, val_generator = create_data_generators()\n\nprint(f\"üìä Classes: {train_generator.class_indices}\")\nprint(f\"üìà Training samples: {train_generator.samples}\")\nprint(f\"üìâ Validation samples: {val_generator.samples}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:47:38.495836Z","iopub.execute_input":"2025-12-09T13:47:38.496162Z","iopub.status.idle":"2025-12-09T13:47:45.869906Z","shell.execute_reply.started":"2025-12-09T13:47:38.496137Z","shell.execute_reply":"2025-12-09T13:47:45.868147Z"}},"outputs":[{"name":"stdout","text":"üîÑ Creating data generators...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/4236155261.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üîÑ Creating data generators...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üìä Classes: {train_generator.class_indices}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/4236155261.py\u001b[0m in \u001b[0;36mcreate_data_generators\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     train_generator = train_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     ):\n\u001b[0;32m-> 1138\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mclasses_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m             \u001b[0mclasses_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"\nimport tensorflow as tf\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\ntf.keras.backend.clear_session()\n\ndef create_enhanced_densenet():\n    base_model = DenseNet121(\n        weights='imagenet',\n        include_top=False,\n        input_shape=(224, 224, 3)\n    )\n    \n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(512, activation='relu', \n              kernel_regularizer=tf.keras.regularizers.l2(0.001))(x) \n    x = Dropout(0.6)(x) \n    x = BatchNormalization()(x)\n    x = Dense(256, activation='relu', \n              kernel_regularizer=tf.keras.regularizers.l2(0.001))(x) \n    x = Dropout(0.5)(x)  \n    x = Dense(128, activation='relu')(x)\n    predictions = Dense(config.NUM_CLASSES, activation='softmax')(x)\n    \n    model = Model(inputs=base_model.input, outputs=predictions)\n    \n    optimizer = Adam(learning_rate=config.LEARNING_RATE)\n    \n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy', 'precision', 'recall']\n    )\n    \n    return model\n\nprint(\"üß† Creating ENHANCED DenseNet model with stronger regularization...\")\nmodel = create_enhanced_densenet()\nprint(\"‚úÖ Enhanced model created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:47:45.871043Z","iopub.status.idle":"2025-12-09T13:47:45.871633Z","shell.execute_reply.started":"2025-12-09T13:47:45.871370Z","shell.execute_reply":"2025-12-09T13:47:45.871394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef create_callbacks():\n    callbacks = [\n        EarlyStopping(\n            monitor='val_loss',\n            patience=2,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=1,\n            min_lr=1e-7,\n            verbose=1\n        ),\n        ModelCheckpoint(\n            'best_densenet_model.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            verbose=1\n        )\n    ]\n    return callbacks\n\nprint(\"‚è∞ Callbacks configured:\")\nprint(\"   - Early Stopping (patience: 7)\")\nprint(\"   - Reduce LR on Plateau\")\nprint(\"   - Model Checkpointing\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:47:45.873468Z","iopub.status.idle":"2025-12-09T13:47:45.873866Z","shell.execute_reply.started":"2025-12-09T13:47:45.873701Z","shell.execute_reply":"2025-12-09T13:47:45.873721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"üéØ Starting model training WITH CLASS BALANCING...\")\n\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\ny_train = train_generator.classes\nclass_weights = compute_class_weight(\n    'balanced',\n    classes=np.unique(y_train),\n    y=y_train\n)\nclass_weight_dict = dict(enumerate(class_weights))\n\nprint(\"üìä Applying class weights to address imbalance:\")\nclass_names = list(train_generator.class_indices.keys())\nfor i, (class_name, weight) in enumerate(zip(class_names, class_weights)):\n    samples = np.sum(y_train == i)\n    print(f\"   {class_name}: {samples} samples ‚Üí weight: {weight:.2f}x\")\n\ncallbacks = create_callbacks()\n\nhistory = model.fit(\n    train_generator,\n    epochs=config.EPOCHS,\n    validation_data=val_generator,\n    callbacks=callbacks,\n    class_weight=class_weight_dict,  \n    verbose=1\n)\n\nprint(\"‚úÖ Training completed with class balancing!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:47:45.875400Z","iopub.status.idle":"2025-12-09T13:47:45.875792Z","shell.execute_reply.started":"2025-12-09T13:47:45.875613Z","shell.execute_reply":"2025-12-09T13:47:45.875630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training history\ndef plot_training_history(history):\n    plt.figure(figsize=(15, 5))\n    \n    # Accuracy plot \n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n    plt.title('Model Accuracy', fontsize=14, fontweight='bold')\n    plt.ylabel('Accuracy', fontsize=12)\n    plt.xlabel('Epoch', fontsize=12)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Loss plot\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n    plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n    plt.title('Model Loss', fontsize=14, fontweight='bold')\n    plt.ylabel('Loss', fontsize=12)\n    plt.xlabel('Epoch', fontsize=12)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(\"üìà Plotting training history...\")\nplot_training_history(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:47:45.877066Z","iopub.status.idle":"2025-12-09T13:47:45.877499Z","shell.execute_reply.started":"2025-12-09T13:47:45.877309Z","shell.execute_reply":"2025-12-09T13:47:45.877328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def comprehensive_evaluation(model, val_generator):\n    \n    val_generator.reset()\n    y_true = val_generator.classes\n    y_pred_proba = model.predict(val_generator, verbose=1)\n    y_pred = np.argmax(y_pred_proba, axis=1)\n    \n    \n    class_labels = list(val_generator.class_indices.keys())\n    \n    # 1. Classification Report\n    print(\"=\" * 60)\n    print(\"COMPREHENSIVE CLASSIFICATION REPORT\")\n    print(\"=\" * 60)\n    print(classification_report(y_true, y_pred, target_names=class_labels))\n    \n    # 2. Confusion Matrix\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    cm = confusion_matrix(y_true, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_labels, yticklabels=class_labels)\n    plt.title('Confusion Matrix', fontweight='bold')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    \n    # 3. Normalized Confusion Matrix\n    plt.subplot(1, 3, 2)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n                xticklabels=class_labels, yticklabels=class_labels)\n    plt.title('Normalized Confusion Matrix', fontweight='bold')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    \n    # 4. ROC Curve (One-vs-Rest for multi-class)\n    plt.subplot(1, 3, 3)\n    y_true_bin = label_binarize(y_true, classes=[0, 1, 2, 3])\n    \n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n    \n    for i in range(config.NUM_CLASSES):\n        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n        plt.plot(fpr[i], tpr[i], label=f'{class_labels[i]} (AUC = {roc_auc[i]:.2f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Multi-class ROC Curve', fontweight='bold')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 5. Additional Metrics\n    accuracy = np.mean(y_true == y_pred)\n    print(f\"\\nüìä Overall Accuracy: {accuracy:.4f}\")\n    \n    # Per-class accuracy\n    class_accuracy = {}\n    print(\"\\nüìà Per-class Accuracy:\")\n    for i, label in enumerate(class_labels):\n        class_mask = y_true == i\n        if np.sum(class_mask) > 0:  # Avoid division by zero\n            class_accuracy[label] = np.mean(y_pred[class_mask] == i)\n            print(f\"   {label}: {class_accuracy[label]:.4f}\")\n    \n    return y_true, y_pred, y_pred_proba\n\nprint(\"üìä Performing comprehensive evaluation...\")\ny_true, y_pred, y_pred_proba = comprehensive_evaluation(model, val_generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:47:45.880353Z","iopub.status.idle":"2025-12-09T13:47:45.880862Z","shell.execute_reply.started":"2025-12-09T13:47:45.880630Z","shell.execute_reply":"2025-12-09T13:47:45.880653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef visualize_predictions(model, val_generator, num_samples=8):\n    val_generator.reset()\n    x_batch, y_batch = next(val_generator)\n    y_pred = model.predict(x_batch)\n    \n    class_labels = list(val_generator.class_indices.keys())\n    \n    plt.figure(figsize=(15, 10))\n    for i in range(num_samples):\n        plt.subplot(2, 4, i+1)\n        plt.imshow(x_batch[i])\n        true_label = class_labels[np.argmax(y_batch[i])]\n        pred_label = class_labels[np.argmax(y_pred[i])]\n        confidence = np.max(y_pred[i])\n        \n        color = 'green' if true_label == pred_label else 'red'\n        plt.title(f'True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2f}', \n                 color=color, fontsize=10, fontweight='bold')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.suptitle('Sample Predictions (Green=Correct, Red=Incorrect)', fontsize=14, fontweight='bold')\n    plt.show()\n\nprint(\"üëÄ Visualizing sample predictions...\")\nvisualize_predictions(model, val_generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:47:45.882599Z","iopub.status.idle":"2025-12-09T13:47:45.883058Z","shell.execute_reply.started":"2025-12-09T13:47:45.882822Z","shell.execute_reply":"2025-12-09T13:47:45.882840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(\"üíæ Saving model...\")\nmodel.save('alzheimer_densenet_final.h5')\n\nfinal_train_acc = history.history['accuracy'][-1]\nfinal_val_acc = history.history['val_accuracy'][-1]\nfinal_train_loss = history.history['loss'][-1]\nfinal_val_loss = history.history['val_loss'][-1]\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"üéØ FINAL TRAINING RESULTS\")\nprint(\"=\"*50)\nprint(f\"üìà Final Training Accuracy: {final_train_acc:.4f}\")\nprint(f\"üìâ Final Validation Accuracy: {final_val_acc:.4f}\")\nprint(f\"üìà Final Training Loss: {final_train_loss:.4f}\")\nprint(f\"üìâ Final Validation Loss: {final_val_loss:.4f}\")\n\nif len(history.history['accuracy']) > 1:\n    initial_acc = history.history['accuracy'][0]\n    improvement = final_train_acc - initial_acc\n    print(f\"üìä Accuracy Improvement: +{improvement:.4f}\")\n\nprint(\"\\n‚úÖ Pipeline completed successfully!\")\nprint(\"üöÄ Model saved as 'alzheimer_densenet_final.h5'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:47:45.884389Z","iopub.status.idle":"2025-12-09T13:47:45.884884Z","shell.execute_reply.started":"2025-12-09T13:47:45.884665Z","shell.execute_reply":"2025-12-09T13:47:45.884686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef additional_analysis():\n    print(\"üîç Additional Analysis for Research Paper:\")\n    print(\"-\" * 50)\n    \n    trainable_params = np.sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n    non_trainable_params = np.sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights])\n    total_params = trainable_params + non_trainable_params\n    \n    print(f\"üìê Model Parameters:\")\n    print(f\"   Trainable: {trainable_params:,}\")\n    print(f\"   Non-trainable: {non_trainable_params:,}\")\n    print(f\"   Total: {total_params:,}\")\n    \n    print(f\"\\nüìä Class Distribution:\")\n    for class_name, class_idx in train_generator.class_indices.items():\n        count = np.sum(y_true == class_idx)\n        print(f\"   {class_name}: {count} samples\")\n\nadditional_analysis()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:47:45.886981Z","iopub.status.idle":"2025-12-09T13:47:45.887471Z","shell.execute_reply.started":"2025-12-09T13:47:45.887255Z","shell.execute_reply":"2025-12-09T13:47:45.887275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"üî¨ Starting ablation experiments...\")\n\ndef build_ablation_model(dropout1=0.6, dropout2=0.5):\n    base_model = DenseNet121(\n        weights='imagenet',\n        include_top=False,\n        input_shape=(224, 224, 3)\n    )\n    for layer in base_model.layers:\n        layer.trainable = False\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(512, activation='relu',\n              kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n    x = Dropout(dropout1)(x)\n    x = BatchNormalization()(x)\n    x = Dense(256, activation='relu',\n              kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n    x = Dropout(dropout2)(x)\n    x = Dense(128, activation='relu')(x)\n    predictions = Dense(config.NUM_CLASSES, activation='softmax')(x)\n    model_ab = Model(inputs=base_model.input, outputs=predictions)\n    optimizer = Adam(learning_rate=config.LEARNING_RATE)\n    model_ab.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy',\n                 tf.keras.metrics.Precision(name='precision'),\n                 tf.keras.metrics.Recall(name='recall')]\n    )\n    return model_ab\n\ndef create_no_aug_generators():\n    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n    train_gen = datagen.flow_from_directory(\n        config.DATA_PATH,\n        target_size=config.IMG_SIZE,\n        batch_size=config.BATCH_SIZE,\n        class_mode='categorical',\n        subset='training'\n    )\n    val_gen = datagen.flow_from_directory(\n        config.DATA_PATH,\n        target_size=config.IMG_SIZE,\n        batch_size=config.BATCH_SIZE,\n        class_mode='categorical',\n        subset='validation',\n        shuffle=False\n    )\n    return train_gen, val_gen\n\ndef run_ablation_experiment(name,\n                            use_class_weights=True,\n                            use_augmentation=True,\n                            dropout1=0.6,\n                            dropout2=0.5,\n                            epochs=2):\n    tf.keras.backend.clear_session()\n    if use_augmentation:\n        train_gen = train_generator\n        val_gen = val_generator\n    else:\n        train_gen, val_gen = create_no_aug_generators()\n    model_ab = build_ablation_model(dropout1=dropout1, dropout2=dropout2)\n    callbacks = create_callbacks()\n    cw = class_weight_dict if use_class_weights else None\n    history_ab = model_ab.fit(\n        train_gen,\n        epochs=epochs,\n        validation_data=val_gen,\n        callbacks=callbacks,\n        class_weight=cw,\n        verbose=1\n    )\n    results = model_ab.evaluate(val_gen, verbose=0)\n    metrics = dict(zip(model_ab.metrics_names, results))\n    metrics[\"experiment\"] = name\n    metrics[\"use_class_weights\"] = use_class_weights\n    metrics[\"use_augmentation\"] = use_augmentation\n    metrics[\"dropout1\"] = dropout1\n    metrics[\"dropout2\"] = dropout2\n    metrics[\"epochs\"] = epochs\n    return metrics, history_ab\n\nbaseline_metrics, baseline_history = run_ablation_experiment(\n    name=\"Baseline_full\",\n    use_class_weights=True,\n    use_augmentation=True,\n    dropout1=0.6,\n    dropout2=0.5,\n    epochs=4\n)\n\nno_cw_metrics, no_cw_history = run_ablation_experiment(\n    name=\"No_class_weights\",\n    use_class_weights=False,\n    use_augmentation=True,\n    dropout1=0.6,\n    dropout2=0.5,\n    epochs=2\n)\n\nno_aug_metrics, no_aug_history = run_ablation_experiment(\n    name=\"No_augmentation\",\n    use_class_weights=True,\n    use_augmentation=False,\n    dropout1=0.6,\n    dropout2=0.5,\n    epochs=2\n)\n\nlow_do_metrics, low_do_history = run_ablation_experiment(\n    name=\"Dropout_0_3\",\n    use_class_weights=True,\n    use_augmentation=True,\n    dropout1=0.3,\n    dropout2=0.3,\n    epochs=2\n)\n\nall_results = [\n    baseline_metrics,\n    no_cw_metrics,\n    no_aug_metrics,\n    low_do_metrics\n]\n\nablation_results_df = pd.DataFrame(all_results)\nprint(ablation_results_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:47:45.889447Z","iopub.status.idle":"2025-12-09T13:47:45.890012Z","shell.execute_reply.started":"2025-12-09T13:47:45.889689Z","shell.execute_reply":"2025-12-09T13:47:45.889710Z"}},"outputs":[],"execution_count":null}]}